{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the target variable distribution\n",
    "print(data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X, y = data['text'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the values to numpy array\n",
    "X = np.reshape(X.values, (X.size,))\n",
    "y =np.reshape(y.values, (y.size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training/testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: \"73rd GOODE Water Ski National Championships will go on as planned next week  http://t.co/PgKBT3MBAp. (Event w/ damage from a tornado on Mon)\", Target variable -> 1.\n",
      "\n",
      "Input sentence: \"The tragedy of life is not that it ends so soon but that we wait so long to begin it. ~ W.M. Lewis #quotes\", Target variable -> 0.\n",
      "\n",
      "Input sentence: \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", Target variable -> 0.\n",
      "\n",
      "Input sentence: \"2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/VDeFmulx43 http://t.co/yqpAIjSa5g\", Target variable -> 0.\n",
      "\n",
      "Input sentence: \"@Rubi_ How many stacks of burning did it apply?\", Target variable -> 0.\n",
      "\n",
      "Input sentence: \"Grow Calgary avoids worst of city's wicked weather * ~ 16 http://t.co/HLyHDfWsQB http://t.co/GwSNBMmcqF\", Target variable -> 1.\n",
      "\n",
      "Input sentence: \"I liked a @YouTube video http://t.co/N95IGskd3p Minecraft: Episode 2 'Blaze Farm Beginnings!'\", Target variable -> 0.\n",
      "\n",
      "Input sentence: \"Just absolutely obliterated a moth my new purchase is boss\", Target variable -> 0.\n",
      "\n",
      "Input sentence: \"We're #hiring! Read about our latest #job opening here: Registered Nurse - Call-in - Military Program - http://t.co/l0hhwB9LSZ #Nursing\", Target variable -> 0.\n",
      "\n",
      "Input sentence: \"I HAVE GOT MORE VIDEOS THAN YOU RAPPERS GOT SONGS! http://t.co/pBLvPM6C27\", Target variable -> 0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 sentences from X_train and corresponding labels from y_train\n",
    "for idx in range(10):\n",
    "    print(f'Input sentence: \"{X_train[idx]}\",', f'Target variable -> {y_train[idx]}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an embedding layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to read glove vectors\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r',encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193514\n",
      "(50,)\n",
      "1193514\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained 50-dimensional GloVe embeddings\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../models/glove.twitter.27B.50d.txt')\n",
    "\n",
    "print(len(word_to_vec_map))\n",
    "print(word_to_vec_map[\"happy\"].shape)\n",
    "print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1 # add 1 row for unknown words\n",
    "    emb_dim = word_to_vec_map[\"happy\"].shape[0]\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word_to_vec_map[word].shape != (50,): \n",
    "#             print(word) #'0.45973' embedding has shape of (49,)\n",
    "            continue \n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct input and output sizes; make it non-trainable\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer\n",
    "    embedding_layer.build((None,)) \n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][:] = [ 9.0566e-01 -7.1792e-01 -1.9574e-01 -8.0743e-01 -2.4903e-02  3.1071e-01\n",
      "  8.9485e-01  6.3035e-01 -3.3863e-01  7.0584e-01  1.2707e-01  3.7673e-01\n",
      " -2.7810e+00  2.5292e-01  5.3043e-02  3.0618e-01 -4.2217e-01 -8.5150e-03\n",
      " -1.1452e+00 -5.1643e-01 -2.3699e-01 -3.1577e-01  2.4883e-01  1.0689e+00\n",
      "  5.5007e-01 -1.2806e+00 -2.4169e-02 -3.1108e-01  1.3964e+00 -9.0377e-01\n",
      " -9.1328e-01  3.4808e-01 -7.5944e-01  9.9209e-01  9.5123e-01  1.0886e-01\n",
      " -1.8141e-01 -4.6055e-01 -8.2691e-01  1.4846e-01 -1.3769e+00 -2.9166e-01\n",
      "  1.0895e-01  6.1422e-01  1.8414e-01  1.5971e-01  7.1934e-02  1.1230e-03\n",
      "  2.8188e-02  3.0385e-01]\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][:] =\", embedding_layer.get_weights()[0][1][:]) #(batch size, max input length, embedding vector size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sentences to array of word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an array of sentences (strings) into an array of indices corresponding to words in the sentences; the output shape should be such that it can be given to Embedding()\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0] #number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m): \n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words; should get a list of words\n",
    "        sentence_words = X[i,].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        # Loop over the words of sentence_words, until hits max_len\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index.keys():\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            else:\n",
    "                X_indices[i, j] = 0 # to handle unknown words\n",
    "            \n",
    "            j += 1\n",
    "            # if j is exceeding max length, then not adding more word index to the array; generates less sparse data\n",
    "            if j == max_len: break \n",
    "\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "max_len_word = max(np.array(X_train), key=len)\n",
    "max_len = len(max_len_word)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['73rd GOODE Water Ski National Championships will go on as planned next week  http://t.co/PgKBT3MBAp. (Event w/ damage from a tornado on Mon)'\n",
      " 'The tragedy of life is not that it ends so soon but that we wait so long to begin it. ~ W.M. Lewis #quotes']\n",
      "X1_indices =\n",
      " [[     0. 234495. 649253. 559866. 401074. 103545. 654140. 232849. 451194.\n",
      "   37724. 482073. 406959. 650502.      0.      0.      0. 136107. 215710.\n",
      "    2115. 610661. 451194.      0.      0.      0.      0.]\n",
      " [601627. 612407. 446383. 341139. 283380. 424732. 601405. 284816. 183147.\n",
      "  563886. 567137.  88334. 601405. 649864. 647422. 563886. 346613. 607687.\n",
      "   60942. 284828. 675654. 646643. 339752.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = X_train[:2]\n",
    "defined_max_len = 25\n",
    "X1_indices = sentences_to_indices(X1, word_to_index, max_len = defined_max_len)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed embedding layer's output to an LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the LSTM network's model graph\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph; dtype 'int32' (as it contains indices, which are integers)\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 64-dimensional hidden state; return a batch of sequences\n",
    "    X = LSTM(units=64, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.2\n",
    "    X = Dropout(rate=0.2)(X)\n",
    "    # Propagate X trough another LSTM layer with 64-dimensional hidden state; return a single hidden state, not a batch of sequences\n",
    "    X = LSTM(units=64, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.2\n",
    "    X = Dropout(rate=0.2)(X)\n",
    "    # Propagate X through a Dense layer with 2 units (target variable classes)\n",
    "    X = Dense(units=2)(X)\n",
    "    # Add a sigmoid activation\n",
    "    X = Activation(activation='sigmoid')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 25, 50)            59675750  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25, 64)            29440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 59,738,344\n",
      "Trainable params: 62,594\n",
      "Non-trainable params: 59,675,750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model((defined_max_len,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with defined loss function, optimizer and evaluation meterics\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target to one hot vector\n",
    "y_oh_train = to_categorical(y_train)\n",
    "y_oh_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from sentences to word indecies\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, defined_max_len)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len=defined_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6090/6090 [==============================] - 12s 2ms/step - loss: 0.5122 - accuracy: 0.7585\n",
      "Epoch 2/50\n",
      "6090/6090 [==============================] - 10s 2ms/step - loss: 0.4636 - accuracy: 0.7956\n",
      "Epoch 3/50\n",
      "6090/6090 [==============================] - 11s 2ms/step - loss: 0.4541 - accuracy: 0.7952\n",
      "Epoch 4/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.4439 - accuracy: 0.8054\n",
      "Epoch 5/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.4331 - accuracy: 0.8057\n",
      "Epoch 6/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.4220 - accuracy: 0.8154\n",
      "Epoch 7/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.4103 - accuracy: 0.8209\n",
      "Epoch 8/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.3977 - accuracy: 0.8252\n",
      "Epoch 9/50\n",
      "6090/6090 [==============================] - 9s 2ms/step - loss: 0.3890 - accuracy: 0.8372\n",
      "Epoch 10/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.3730 - accuracy: 0.8377\n",
      "Epoch 11/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.3567 - accuracy: 0.8488\n",
      "Epoch 12/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.3485 - accuracy: 0.8535\n",
      "Epoch 13/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.3218 - accuracy: 0.8663\n",
      "Epoch 14/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.3105 - accuracy: 0.8759\n",
      "Epoch 15/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.2817 - accuracy: 0.8865\n",
      "Epoch 16/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.2691 - accuracy: 0.8961\n",
      "Epoch 17/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.2502 - accuracy: 0.8999\n",
      "Epoch 18/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.2236 - accuracy: 0.9149\n",
      "Epoch 19/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.2114 - accuracy: 0.9198\n",
      "Epoch 20/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.1961 - accuracy: 0.9247\n",
      "Epoch 21/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.1789 - accuracy: 0.9365\n",
      "Epoch 22/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.1649 - accuracy: 0.9425\n",
      "Epoch 23/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.1538 - accuracy: 0.9453\n",
      "Epoch 24/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.1402 - accuracy: 0.9510\n",
      "Epoch 25/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.1378 - accuracy: 0.9510\n",
      "Epoch 26/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.1240 - accuracy: 0.9569\n",
      "Epoch 27/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.1229 - accuracy: 0.9567\n",
      "Epoch 28/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.1108 - accuracy: 0.9600\n",
      "Epoch 29/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0997 - accuracy: 0.9669\n",
      "Epoch 30/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0934 - accuracy: 0.9670\n",
      "Epoch 31/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.1196 - accuracy: 0.9569\n",
      "Epoch 32/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0924 - accuracy: 0.9664\n",
      "Epoch 33/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0885 - accuracy: 0.9681\n",
      "Epoch 34/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0997 - accuracy: 0.9622\n",
      "Epoch 35/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0788 - accuracy: 0.9708\n",
      "Epoch 36/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0729 - accuracy: 0.9720\n",
      "Epoch 37/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0710 - accuracy: 0.9745\n",
      "Epoch 38/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0719 - accuracy: 0.9701\n",
      "Epoch 39/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0905 - accuracy: 0.9649\n",
      "Epoch 40/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0739 - accuracy: 0.9732\n",
      "Epoch 41/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0620 - accuracy: 0.9778\n",
      "Epoch 42/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0755 - accuracy: 0.9718\n",
      "Epoch 43/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0673 - accuracy: 0.9722\n",
      "Epoch 44/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0557 - accuracy: 0.9764\n",
      "Epoch 45/50\n",
      "6090/6090 [==============================] - 7s 1ms/step - loss: 0.0612 - accuracy: 0.9756\n",
      "Epoch 46/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0844 - accuracy: 0.9667\n",
      "Epoch 47/50\n",
      "6090/6090 [==============================] - 8s 1ms/step - loss: 0.0611 - accuracy: 0.9747\n",
      "Epoch 48/50\n",
      "6090/6090 [==============================] - 512s 84ms/step - loss: 0.0579 - accuracy: 0.9754\n",
      "Epoch 49/50\n",
      "6090/6090 [==============================] - 12s 2ms/step - loss: 0.0736 - accuracy: 0.9688\n",
      "Epoch 50/50\n",
      "6090/6090 [==============================] - 9s 1ms/step - loss: 0.0561 - accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x146665550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, y_oh_train, epochs = 50, batch_size = 30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523/1523 [==============================] - 1s 454us/step\n",
      "Test loss =  1.158551921988504\n",
      "Test accuracy =  0.7931713461875916\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "loss, acc = model.evaluate(X_test_indices, y_oh_test)\n",
    "print(\"Test loss = \", loss)\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data\n",
    "pred = model.predict(X_test_indices)\n",
    "\n",
    "y_pred = [np.argmax(p) for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793827971109652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82       841\n",
      "           1       0.82      0.69      0.75       682\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.80      0.78      0.79      1523\n",
      "weighted avg       0.80      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: \"Whereas Jez will obliterate the national debt - and give lots of new benefits - by simply printing money! Genius! https://t.co/ReffbkVG9R\"\n",
      "Input word indices: [652676. 294693. 654140. 444770. 601627. 401074. 139581.   1743.  26338.\n",
      " 231380. 347738. 446383. 406520.  63204.   1743.  89093. 557140. 493167.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 1, Predicted -> 0.\n",
      "\n",
      "Input sentence: \"I think bombing Iran would be kinder... https://t.co/GVm70U2bPm\"\n",
      "Input word indices: [266801. 602685.  77716. 282695. 657158.  59105.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 0, Predicted -> 1.\n",
      "\n",
      "Input sentence: \"Our thoughts are with these local residents! Time for some heavy rain!!! http://t.co/x3g2OX6K8R\"\n",
      "Input word indices: [456923. 603259.  34878. 655002. 602272. 345525.      0. 605075. 211804.\n",
      " 566117. 253634.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 1, Predicted -> 0.\n",
      "\n",
      "Input sentence: \"*screams in 25 different languages*\"\n",
      "Input word indices: [     0. 273726.      0. 152906.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 0, Predicted -> 1.\n",
      "\n",
      "Input sentence: \"ok peace I hope I fall off a cliff along with my dignity\"\n",
      "Input word indices: [447936. 468466. 266801. 261714. 266801. 199899. 446492.   2115. 112529.\n",
      "  21673. 655002. 393680. 153464.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 1, Predicted -> 0.\n",
      "\n",
      "Input sentence: \"on the outside you're ablaze and alive\n",
      "but you're dead inside\"\n",
      "Input word indices: [451194. 601627. 457239.      0.   4544.  26338.  20327.  88334.      0.\n",
      " 139142. 278643.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 0, Predicted -> 1.\n",
      "\n",
      "Input sentence: \"@Beyonce @NicoleKidman @Oprah these money grubbing kikes need to get a clueI have no money but I can still destroy with telekinesis. Watch.\"\n",
      "Input word indices: [     0.      0.      0. 602272. 385153. 238931.      0. 404137. 607687.\n",
      " 227866.   2115.      0. 251960. 422256. 385153.  88334. 266801.  94350.\n",
      " 574708. 148572. 655002.      0.      0.      0.      0.],\n",
      "Expected -> 1, Predicted -> 0.\n",
      "\n",
      "Input sentence: \"Newlyweds feed thousands of Syrian refugees instead of hosting a banquet wedding dinner http://t.co/EGcv7ybjae #Age #news\"\n",
      "Input word indices: [4.06672e+05 2.03342e+05 6.03285e+05 4.46383e+05 5.84192e+05 5.12757e+05\n",
      " 2.79314e+05 4.46383e+05 2.62374e+05 2.11500e+03 5.44290e+04 6.50271e+05\n",
      " 1.55982e+05 0.00000e+00 0.00000e+00 4.29000e+02 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 0.00000e+00],\n",
      "Expected -> 0, Predicted -> 1.\n",
      "\n",
      "Input sentence: \"Ready for my close up... Errrr nope!! #notgoingoutinthat #hailstorm #alberta @HellOnWheelsAMC @HoW_fans @TalkingHell http://t.co/9gIAXD6JTY\"\n",
      "Input word indices: [509170. 211804. 393680. 112820.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 1, Predicted -> 0.\n",
      "\n",
      "Input sentence: \"Campsite recommendations \n",
      "Toilets /shower \n",
      "Pub \n",
      "Fires \n",
      "No kids \n",
      "Pizza shop \n",
      "Forest \n",
      "Pretty stream \n",
      "No midges\n",
      "No snakes\n",
      "Thanks ??\"\n",
      "Input word indices: [ 94258. 510890. 608485.      0. 497846. 207623. 422256. 315831. 481280.\n",
      " 553612. 212014. 492367. 575836. 422256. 377577. 422256. 563148. 601227.\n",
      "      0.      0.      0.      0.      0.      0.      0.],\n",
      "Expected -> 1, Predicted -> 0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Error analysis, for first 10 errors\n",
    "counter = 0\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    if y_pred[i] != y_test[i] and counter < 10:\n",
    "        print(f'Input sentence: \"{X_test[i]}\"')\n",
    "        print(f'Input word indices: {x[i]},') # to check if unknown words are too much; or too sparse vectors\n",
    "        print(f'Expected -> {y_test[i]}, Predicted -> {y_pred[i]}.\\n')\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit497e650d671a465a832873f585116d0b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
